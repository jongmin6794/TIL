{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "표제어(Lemmatization) 추출:서로 형태는 다르지만, root 단어를\n",
    "가지고 비교해서, 전체적으로 단어의 개수를 줄이자.\n",
    "am, are, is, was, were.. => be(표제어)\n",
    "\n",
    "형태소 : stem(어간, 단어의 의미), affix(접사,부가적 의미)\n",
    "형태소 파싱 : 어간, 접사를 분리하는 작업\n",
    "dog(독립형태소)\n",
    "dogs = dog(어간) + s(접사)\n",
    "\n",
    "WordNetLemmatizer :  NLTK에 표제어 추출 도구\n",
    "\"\"\"\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "wnl=WordNetLemmatizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'die'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#wnl.lemmatize('watched') #watched\n",
    "wnl.lemmatize('watched', 'v')\n",
    "wnl.lemmatize('has','v')\n",
    "wnl.lemmatize('dies','v')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#어간 추출\n",
    "text=\"Python is an interpreted, high-level, general-purpose programming language.\"\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.tokenize import word_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Python', 'is', 'an', 'interpreted', ',', 'high-level', ',', 'general-purpose', 'programming', 'language', '.']\n"
     ]
    }
   ],
   "source": [
    "ps=PorterStemmer()\n",
    "words=word_tokenize(text)\n",
    "print(words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['python', 'is', 'an', 'interpret', ',', 'high-level', ',', 'general-purpos', 'program', 'languag', '.']\n"
     ]
    }
   ],
   "source": [
    "print([ps.stem(w) for w in words])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "electric\n",
      "formal\n"
     ]
    }
   ],
   "source": [
    "print(ps.stem('electricical'))\n",
    "print(ps.stem('formalize'))\n",
    "#구글 : 마틴 포터 or 포터스태머 검색"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "ps.stem('going') #go\n",
    "ps.stem('gone') #gone\n",
    "from nltk.stem import LancasterStemmer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'die'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ls=LancasterStemmer()\n",
    "ls.stem('going') #going\n",
    "ls.stem('gone') #gon\n",
    "ls.stem('dies')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#불용어:stopwords\n",
    "from nltk.corpus import stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Family', 'is', 'not', 'an', 'important', 'thing', '.', 'It', \"'s\", 'everything']\n",
      "['Family', 'important', 'thing', '.', 'It', \"'s\", 'everything']\n"
     ]
    }
   ],
   "source": [
    "sw=stopwords.words('english')\n",
    "ex=\"Family is not an important thing. It's everything\"\n",
    "wt=word_tokenize(ex)\n",
    "res=[]\n",
    "for w in wt:\n",
    "    if w not in sw:\n",
    "        res.append(w)\n",
    "print(wt)\n",
    "print(res)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['최근', '코로나19로', '인한', '감염으로', '인해', '확진자', '및', '사망자가', '증가하고', '있습니다', '.', '코로나19를', '이겨냅시다', '.']\n",
      "['코로나19로', '감염으로', '인해', '확진자', '사망자가', '증가하고', '있습니다', '.', '코로나19를', '이겨냅시다', '.']\n"
     ]
    }
   ],
   "source": [
    "#www.ranks.nl/stopwords/korean\n",
    "ex=\"\"\"\n",
    "최근 코로나19로 인한 감염으로 인해 \n",
    "확진자 및 사망자가 증가하고 있습니다. \n",
    "코로나19를 이겨냅시다.\n",
    "\"\"\"\n",
    "stop_words=\"인한 증가 최근 및\"\n",
    "stop_words=stop_words.split(\" \")\n",
    "wt=word_tokenize(ex)\n",
    "print(wt)\n",
    "res=[]\n",
    "for w in wt:\n",
    "    if w not in stop_words:\n",
    "        res.append(w)        \n",
    "print(res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Python is an interpreted, high-level, general-purpose programming language.',\n",
       " \"Created by Guido van Rossum and first released in 1991, Python's design philosophy emphasizes code readability with its notable use of significant whitespace.\",\n",
       " 'Its language constructs and object-oriented approach aim to help programmers write clear, logical code for small and large-scale projects.']"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.tokenize import sent_tokenize\n",
    "text=\"Python is an interpreted, high-level, general-purpose programming language. Created by Guido van Rossum and first released in 1991, Python's design philosophy emphasizes code readability with its notable use of significant whitespace. Its language constructs and object-oriented approach aim to help programmers write clear, logical code for small and large-scale projects.\"\n",
    "text=sent_tokenize(text) #3개 문장\n",
    "text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['python', 'interpreted', 'high-level', 'general-purpose', 'programming', 'language'], ['created', 'guido', 'van', 'rossum', 'first', 'released', '1991', 'python', 'design', 'philosophy', 'emphasizes', 'code', 'readability', 'notable', 'use', 'significant', 'whitespace'], ['language', 'constructs', 'object-oriented', 'approach', 'aim', 'help', 'programmers', 'write', 'clear', 'logical', 'code', 'small', 'large-scale', 'projects']]\n"
     ]
    }
   ],
   "source": [
    "#모든 단어를 소문자, 불용어 제거, 길이가 2이하 제거\n",
    "#print(sw)\n",
    "\n",
    "voc={}\n",
    "sentences=[]\n",
    "for t in text:\n",
    "    words=word_tokenize(t)\n",
    "    res=[]\n",
    "    for word in words:\n",
    "        word=word.lower()\n",
    "        if word not in sw:\n",
    "            if len(word)>2:\n",
    "                res.append(word)\n",
    "                if word not in voc:\n",
    "                    voc[word]=0\n",
    "                voc[word]+=1\n",
    "    sentences.append(res)\n",
    "print(sentences) #[[문장1][문장2],[문장3]]\n",
    "#voc=={'python':3, ...}        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('python', 2),\n",
       " ('language', 2),\n",
       " ('code', 2),\n",
       " ('interpreted', 1),\n",
       " ('high-level', 1),\n",
       " ('general-purpose', 1),\n",
       " ('programming', 1),\n",
       " ('created', 1),\n",
       " ('guido', 1),\n",
       " ('van', 1),\n",
       " ('rossum', 1),\n",
       " ('first', 1),\n",
       " ('released', 1),\n",
       " ('1991', 1),\n",
       " ('design', 1),\n",
       " ('philosophy', 1),\n",
       " ('emphasizes', 1),\n",
       " ('readability', 1),\n",
       " ('notable', 1),\n",
       " ('use', 1),\n",
       " ('significant', 1),\n",
       " ('whitespace', 1),\n",
       " ('constructs', 1),\n",
       " ('object-oriented', 1),\n",
       " ('approach', 1),\n",
       " ('aim', 1),\n",
       " ('help', 1),\n",
       " ('programmers', 1),\n",
       " ('write', 1),\n",
       " ('clear', 1),\n",
       " ('logical', 1),\n",
       " ('small', 1),\n",
       " ('large-scale', 1),\n",
       " ('projects', 1)]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vs=sorted(voc.items(), key=lambda x:x[1], reverse=True)\n",
    "vs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'python': 1, 'language': 2, 'code': 3}\n"
     ]
    }
   ],
   "source": [
    "wi={}\n",
    "i=0\n",
    "for w,f in vs:\n",
    "    if f>1 :\n",
    "        i+=1\n",
    "        wi[w]=i #index 부여\n",
    "print(wi)\n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "2\n",
      "3\n"
     ]
    }
   ],
   "source": [
    "a=wi.items()\n",
    "for w,i in a:\n",
    "    print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['code']\n"
     ]
    }
   ],
   "source": [
    "vocSize=2 #가장 많이 언급된 2개 단어만 추출\n",
    "\n",
    "#단어의 인덱스가 vocSize를 초과하는 단어 추출\n",
    "wordFreq=[w for w,i in wi.items() if i>vocSize]\n",
    "print(wordFreq)\n",
    "for w in wordFreq:\n",
    "    del wi[w]\n",
    "#인덱스(index)가 3번 이상인 단어는 제거(1번, 2번만 추출)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'python': 1, 'language': 2}"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "#OOV(out of voc, 단어집합에 없는 단어)\n",
    "# 영수:철수야 안녕? (입력 데이터, x)\n",
    "# 철수:응 너도 안녕.(출력 데이터, y)\n",
    "# ...\n",
    "# 철수야 안녕? -> 모델 -> 응 너도 안녕.\n",
    "\n",
    "# 철수(1) 안녕(2)  ->     -> 응  너  안녕\n",
    "# ...\n",
    "\n",
    "# 철수 안녕 너 응 잘가 ...\n",
    "#   1    2   3   4   5 ...\n",
    "\n",
    "# 개체명인식"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['python',\n",
       "  'interpreted',\n",
       "  'high-level',\n",
       "  'general-purpose',\n",
       "  'programming',\n",
       "  'language'],\n",
       " ['created',\n",
       "  'guido',\n",
       "  'van',\n",
       "  'rossum',\n",
       "  'first',\n",
       "  'released',\n",
       "  '1991',\n",
       "  'python',\n",
       "  'design',\n",
       "  'philosophy',\n",
       "  'emphasizes',\n",
       "  'code',\n",
       "  'readability',\n",
       "  'notable',\n",
       "  'use',\n",
       "  'significant',\n",
       "  'whitespace'],\n",
       " ['language',\n",
       "  'constructs',\n",
       "  'object-oriented',\n",
       "  'approach',\n",
       "  'aim',\n",
       "  'help',\n",
       "  'programmers',\n",
       "  'write',\n",
       "  'clear',\n",
       "  'logical',\n",
       "  'code',\n",
       "  'small',\n",
       "  'large-scale',\n",
       "  'projects']]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "#원핫인코딩\n",
    "from konlpy.tag import Okt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "okt=Okt()\n",
    "tok=okt.morphs(\"나는 자연어 처리를 학습한다\")\n",
    "#원핫벡터:단어 집합을 벡터로 표현하는 방식"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'나': 0, '는': 1, '자연어': 2, '처리': 3, '를': 4, '학습': 5, '한다': 6}\n"
     ]
    }
   ],
   "source": [
    "w2i={}\n",
    "for v in tok:\n",
    "    if v not in w2i.keys():\n",
    "        w2i[v]=len(w2i)\n",
    "print(w2i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0, 0, 1, 0, 0, 0, 0]\n"
     ]
    }
   ],
   "source": [
    "#'자연어' -> 원핫 -> 0010000\n",
    "def ohe(w, w2i):\n",
    "    ohv=[0]*len(w2i)\n",
    "    index=w2i[w]\n",
    "    ohv[index]=1\n",
    "    return ohv\n",
    "print(ohe(\"자연어\", w2i))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'판다스': 1, '데이터': 2, '분석은': 3, '최고야': 4, '곰이야': 5}\n"
     ]
    }
   ],
   "source": [
    "#케라스 원핫인코딩 : to_categorical()\n",
    "text=\"데이터 분석은 판다스 최고야 판다스 곰이야\"\n",
    "\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "tok=Tokenizer()\n",
    "tok.fit_on_texts([text])\n",
    "print(tok.word_index)\n",
    "#단어집합(voc)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample=\"판다스 분석은 동물원에서 한다\"\n",
    "enc=tok.texts_to_sequences([sample])[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0., 1., 0., 0.],\n",
       "       [0., 0., 0., 1.]], dtype=float32)"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "to_categorical(enc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "#단어 분리(BPE) => 기계번역\n",
    "#학습과정에서 사용되지 않은 단어가 테스트과정에서\n",
    "#입력되면 -> OOV 문제 => 제대로 모델이 동작X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# run-length 기법  aaaabbbaaaaa => a4b3a5\n",
    "# 허프만 트리(인코딩)를 이용한 압축\n",
    "#a=> 101, b=10, c=1101...\n",
    "#BPE 압축 알고리즘 => 단어 분리에 응용"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "#AAABDAAABAC\n",
    "#BPE 압축\n",
    "#연속적인 글자 쌍(2글자)을 구성했을때 \n",
    "#가장 많이 등장\n",
    "#1) AA가 가장 많이 등장 => 다른 글자로 치환\n",
    "#=> 소문자 z로 치환\n",
    "#zABDzABAC\n",
    "\n",
    "#2) AB가 가장 많이 등장 => 다른 글자로 치환\n",
    "#=> 소문자 y로 치환\n",
    "#zyDzyAC\n",
    "\n",
    "#3) zy가 가장 많이 등장 => 다른 글자로 치환\n",
    "#=> 소문자 x로 치환\n",
    "#xDxAC\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#BPE : 단어 분리 알고리즘 => 글자 단위 -> 단어 집합\n",
    "\n",
    "# ex)train data\n",
    "# ={low:5, lower:2, newest:6, widest:3}\n",
    "# 1)단어 집합(VOC)\n",
    "# =>low, lower, newest,widest\n",
    "# ex)lowest(테스트 과정) 입력 -> OOV 문제 \n",
    "# 2)BPE 알고리즘(OOV 문제 해결)\n",
    "#    lowest와 lower 같은 의미 단어로 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "### 토픽 모델링 => LSA\n",
    "from sklearn.datasets import fetch_20newsgroups"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset=fetch_20newsgroups(shuffle=True,random_state=1,remove=(\"headers\",\"footers\",\"quotes\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'sklearn.utils.Bunch'>\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "11314"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(type(dataset))\n",
    "documents=dataset.data\n",
    "len(documents)# 11314건의 뉴스 기사"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"Well i'm not sure about the story nad it did seem biased. What\\nI disagree with is your statement that the U.S. Media is out to\\nruin Israels reputation. That is rediculous. The U.S. media is\\nthe most pro-israeli media in the world. Having lived in Europe\\nI realize that incidences such as the one described in the\\nletter have occured. The U.S. media as a whole seem to try to\\nignore them. The U.S. is subsidizing Israels existance and the\\nEuropeans are not (at least not to the same degree). So I think\\nthat might be a reason they report more clearly on the\\natrocities.\\n\\tWhat is a shame is that in Austria, daily reports of\\nthe inhuman acts commited by Israeli soldiers and the blessing\\nreceived from the Government makes some of the Holocaust guilt\\ngo away. After all, look how the Jews are treating other races\\nwhen they got power. It is unfortunate.\\n\""
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(documents)\n",
    "documents[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['alt.atheism',\n",
       " 'comp.graphics',\n",
       " 'comp.os.ms-windows.misc',\n",
       " 'comp.sys.ibm.pc.hardware',\n",
       " 'comp.sys.mac.hardware',\n",
       " 'comp.windows.x',\n",
       " 'misc.forsale',\n",
       " 'rec.autos',\n",
       " 'rec.motorcycles',\n",
       " 'rec.sport.baseball',\n",
       " 'rec.sport.hockey',\n",
       " 'sci.crypt',\n",
       " 'sci.electronics',\n",
       " 'sci.med',\n",
       " 'sci.space',\n",
       " 'soc.religion.christian',\n",
       " 'talk.politics.guns',\n",
       " 'talk.politics.mideast',\n",
       " 'talk.politics.misc',\n",
       " 'talk.religion.misc']"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset.target_names#뉴스 카테고리"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\n\\n\\n\\n\\n\\n\\nYeah, do you expect people to read the FAQ, etc. and actually accept hard\\natheism?  No, you need a little leap of faith, Jimmy.  Your logic runs out\\nof steam!\\n\\n\\n\\n\\n\\n\\n\\nJim,\\n\\nSorry I can't pity you, Jim.  And I'm sorry that you have these feelings of\\ndenial about the faith you need to get by.  Oh well, just pretend that it will\\nall end happily ever after anyway.  Maybe if you start a new newsgroup,\\nalt.atheist.hard, you won't be bummin' so much?\\n\\n\\n\\n\\n\\n\\nBye-Bye, Big Jim.  Don't forget your Flintstone's Chewables!  :) \\n--\\nBake Timmons, III\""
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "documents[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 토픽과 가장 관련성이 높은 단어를 10개씩 출력\n",
    "# topic1:\n",
    "# ~\n",
    "# topic20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>document</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>Well i'm not sure about the story nad it did s...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>\\n\\n\\n\\n\\n\\n\\nYeah, do you expect people to re...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>Although I realize that principle is not one o...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>Notwithstanding all the legitimate fuss about ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>Well, I will have to change the scoring on my ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11309</td>\n",
       "      <td>Danny Rubenstein, an Israeli journalist, will ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11310</td>\n",
       "      <td>\\n</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11311</td>\n",
       "      <td>\\nI agree.  Home runs off Clemens are always m...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11312</td>\n",
       "      <td>I used HP DeskJet with Orange Micros Grappler ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11313</td>\n",
       "      <td>^^^^^^\\n...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>11314 rows × 1 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                document\n",
       "0      Well i'm not sure about the story nad it did s...\n",
       "1      \\n\\n\\n\\n\\n\\n\\nYeah, do you expect people to re...\n",
       "2      Although I realize that principle is not one o...\n",
       "3      Notwithstanding all the legitimate fuss about ...\n",
       "4      Well, I will have to change the scoring on my ...\n",
       "...                                                  ...\n",
       "11309  Danny Rubenstein, an Israeli journalist, will ...\n",
       "11310                                                 \\n\n",
       "11311  \\nI agree.  Home runs off Clemens are always m...\n",
       "11312  I used HP DeskJet with Orange Micros Grappler ...\n",
       "11313                                        ^^^^^^\\n...\n",
       "\n",
       "[11314 rows x 1 columns]"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "newsDf=pd.DataFrame({'document':documents})\n",
    "newsDf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "#특수문제 제거(영문자를 제외)\n",
    "newsDf['clean_doc']=newsDf['document'].str.replace(\"[^a-zA-Z]\",\" \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>document</th>\n",
       "      <th>clean_doc</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>Well i'm not sure about the story nad it did s...</td>\n",
       "      <td>Well i m not sure about the story nad it did s...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>\\n\\n\\n\\n\\n\\n\\nYeah, do you expect people to re...</td>\n",
       "      <td>Yeah  do you expect people to read the ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>Although I realize that principle is not one o...</td>\n",
       "      <td>Although I realize that principle is not one o...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>Notwithstanding all the legitimate fuss about ...</td>\n",
       "      <td>Notwithstanding all the legitimate fuss about ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>Well, I will have to change the scoring on my ...</td>\n",
       "      <td>Well  I will have to change the scoring on my ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11309</td>\n",
       "      <td>Danny Rubenstein, an Israeli journalist, will ...</td>\n",
       "      <td>Danny Rubenstein  an Israeli journalist  will ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11310</td>\n",
       "      <td>\\n</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11311</td>\n",
       "      <td>\\nI agree.  Home runs off Clemens are always m...</td>\n",
       "      <td>I agree   Home runs off Clemens are always me...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11312</td>\n",
       "      <td>I used HP DeskJet with Orange Micros Grappler ...</td>\n",
       "      <td>I used HP DeskJet with Orange Micros Grappler ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11313</td>\n",
       "      <td>^^^^^^\\n...</td>\n",
       "      <td>N...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>11314 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                document  \\\n",
       "0      Well i'm not sure about the story nad it did s...   \n",
       "1      \\n\\n\\n\\n\\n\\n\\nYeah, do you expect people to re...   \n",
       "2      Although I realize that principle is not one o...   \n",
       "3      Notwithstanding all the legitimate fuss about ...   \n",
       "4      Well, I will have to change the scoring on my ...   \n",
       "...                                                  ...   \n",
       "11309  Danny Rubenstein, an Israeli journalist, will ...   \n",
       "11310                                                 \\n   \n",
       "11311  \\nI agree.  Home runs off Clemens are always m...   \n",
       "11312  I used HP DeskJet with Orange Micros Grappler ...   \n",
       "11313                                        ^^^^^^\\n...   \n",
       "\n",
       "                                               clean_doc  \n",
       "0      Well i m not sure about the story nad it did s...  \n",
       "1             Yeah  do you expect people to read the ...  \n",
       "2      Although I realize that principle is not one o...  \n",
       "3      Notwithstanding all the legitimate fuss about ...  \n",
       "4      Well  I will have to change the scoring on my ...  \n",
       "...                                                  ...  \n",
       "11309  Danny Rubenstein  an Israeli journalist  will ...  \n",
       "11310                                                     \n",
       "11311   I agree   Home runs off Clemens are always me...  \n",
       "11312  I used HP DeskJet with Orange Micros Grappler ...  \n",
       "11313                                               N...  \n",
       "\n",
       "[11314 rows x 2 columns]"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "newsDf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "#3글자 이하 제거, 소문자 변환\n",
    "newsDf['clean_doc']=newsDf['clean_doc'].apply(lambda x:' '.join([w for w in x.split() if len(w)>3]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "#소문자 변환\n",
    "newsDf['clean_doc']=newsDf['clean_doc'].apply(lambda x:x.lower())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>document</th>\n",
       "      <th>clean_doc</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>Well i'm not sure about the story nad it did s...</td>\n",
       "      <td>well sure about story seem biased what disagre...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>\\n\\n\\n\\n\\n\\n\\nYeah, do you expect people to re...</td>\n",
       "      <td>yeah expect people read actually accept hard a...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>Although I realize that principle is not one o...</td>\n",
       "      <td>although realize that principle your strongest...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>Notwithstanding all the legitimate fuss about ...</td>\n",
       "      <td>notwithstanding legitimate fuss about this pro...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>Well, I will have to change the scoring on my ...</td>\n",
       "      <td>well will have change scoring playoff pool unf...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11309</td>\n",
       "      <td>Danny Rubenstein, an Israeli journalist, will ...</td>\n",
       "      <td>danny rubenstein israeli journalist will speak...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11310</td>\n",
       "      <td>\\n</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11311</td>\n",
       "      <td>\\nI agree.  Home runs off Clemens are always m...</td>\n",
       "      <td>agree home runs clemens always memorable kinda...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11312</td>\n",
       "      <td>I used HP DeskJet with Orange Micros Grappler ...</td>\n",
       "      <td>used deskjet with orange micros grappler syste...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11313</td>\n",
       "      <td>^^^^^^\\n...</td>\n",
       "      <td>argument with murphy scared hell when came las...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>11314 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                document  \\\n",
       "0      Well i'm not sure about the story nad it did s...   \n",
       "1      \\n\\n\\n\\n\\n\\n\\nYeah, do you expect people to re...   \n",
       "2      Although I realize that principle is not one o...   \n",
       "3      Notwithstanding all the legitimate fuss about ...   \n",
       "4      Well, I will have to change the scoring on my ...   \n",
       "...                                                  ...   \n",
       "11309  Danny Rubenstein, an Israeli journalist, will ...   \n",
       "11310                                                 \\n   \n",
       "11311  \\nI agree.  Home runs off Clemens are always m...   \n",
       "11312  I used HP DeskJet with Orange Micros Grappler ...   \n",
       "11313                                        ^^^^^^\\n...   \n",
       "\n",
       "                                               clean_doc  \n",
       "0      well sure about story seem biased what disagre...  \n",
       "1      yeah expect people read actually accept hard a...  \n",
       "2      although realize that principle your strongest...  \n",
       "3      notwithstanding legitimate fuss about this pro...  \n",
       "4      well will have change scoring playoff pool unf...  \n",
       "...                                                  ...  \n",
       "11309  danny rubenstein israeli journalist will speak...  \n",
       "11310                                                     \n",
       "11311  agree home runs clemens always memorable kinda...  \n",
       "11312  used deskjet with orange micros grappler syste...  \n",
       "11313  argument with murphy scared hell when came las...  \n",
       "\n",
       "[11314 rows x 2 columns]"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "newsDf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "#불용어 제거\n",
    "sw=stopwords.words('english')\n",
    "#토큰화\n",
    "tokenizedDoc=newsDf['clean_doc'].apply(lambda x:x.split())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizedDoc=tokenizedDoc.apply(lambda x:[item for item in x if item not in sw])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['yeah',\n",
       " 'expect',\n",
       " 'people',\n",
       " 'read',\n",
       " 'actually',\n",
       " 'accept',\n",
       " 'hard',\n",
       " 'atheism',\n",
       " 'need',\n",
       " 'little',\n",
       " 'leap',\n",
       " 'faith',\n",
       " 'jimmy',\n",
       " 'logic',\n",
       " 'runs',\n",
       " 'steam',\n",
       " 'sorry',\n",
       " 'pity',\n",
       " 'sorry',\n",
       " 'feelings',\n",
       " 'denial',\n",
       " 'faith',\n",
       " 'need',\n",
       " 'well',\n",
       " 'pretend',\n",
       " 'happily',\n",
       " 'ever',\n",
       " 'anyway',\n",
       " 'maybe',\n",
       " 'start',\n",
       " 'newsgroup',\n",
       " 'atheist',\n",
       " 'hard',\n",
       " 'bummin',\n",
       " 'much',\n",
       " 'forget',\n",
       " 'flintstone',\n",
       " 'chewables',\n",
       " 'bake',\n",
       " 'timmons']"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizedDoc[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'yeah expect people read actually accept hard atheism need little leap faith jimmy your logic runs steam sorry pity sorry that have these feelings denial about faith need well just pretend that will happily ever after anyway maybe start newsgroup atheist hard bummin much forget your flintstone chewables bake timmons'"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# TF-IDF 매트릭스 구성\n",
    "# TFIDF는 토큰화가 안되어 있는 텍스트 데이터로 구성\n",
    "# 토큰화 <-> 역토큰화(토큰화 취소)\n",
    "newsDf['clean_doc'][1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "#역 토큰화\n",
    "deTokenizedDoc=[]\n",
    "for i in range(len(newsDf)):\n",
    "    temp=' '.join(tokenizedDoc[i])\n",
    "    deTokenizedDoc.append(temp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'yeah expect people read actually accept hard atheism need little leap faith jimmy your logic runs steam sorry pity sorry that have these feelings denial about faith need well just pretend that will happily ever after anyway maybe start newsgroup atheist hard bummin much forget your flintstone chewables bake timmons'"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "newsDf['clean_doc'][1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(11314, 1000)"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#tfidf 행렬 구성\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "vector=TfidfVectorizer(stop_words='english',max_features=1000)#1000개 단어\n",
    "res=vector.fit_transform(newsDf['clean_doc'])\n",
    "res.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "#svd(full,truncated)\n",
    "#특이값분해\n",
    "#행렬=U*S*VT\n",
    "#절단된 SVD -> 차원 축소\n",
    "#"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "#토픽 숫자:n_components\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "svdModel=TruncatedSVD(n_components=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TruncatedSVD(algorithm='randomized', n_components=20, n_iter=5,\n",
       "             random_state=None, tol=0.0)"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "svdModel.fit(res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(20, 1000)"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "np.shape(svdModel.components_)\n",
    "#20개의 토픽과 1000개의 단어"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['ability',\n",
       " 'able',\n",
       " 'accept',\n",
       " 'access',\n",
       " 'according',\n",
       " 'account',\n",
       " 'action',\n",
       " 'actions',\n",
       " 'actual',\n",
       " 'actually',\n",
       " 'added',\n",
       " 'addition',\n",
       " 'additional',\n",
       " 'address',\n",
       " 'administration',\n",
       " 'advance',\n",
       " 'advice',\n",
       " 'agencies',\n",
       " 'agree',\n",
       " 'algorithm',\n",
       " 'allow',\n",
       " 'allowed',\n",
       " 'allows',\n",
       " 'amendment',\n",
       " 'america',\n",
       " 'american',\n",
       " 'americans',\n",
       " 'analysis',\n",
       " 'angeles',\n",
       " 'anonymous',\n",
       " 'answer',\n",
       " 'answers',\n",
       " 'anti',\n",
       " 'anybody',\n",
       " 'apparently',\n",
       " 'appear',\n",
       " 'appears',\n",
       " 'apple',\n",
       " 'application',\n",
       " 'applications',\n",
       " 'apply',\n",
       " 'appreciate',\n",
       " 'appreciated',\n",
       " 'approach',\n",
       " 'appropriate',\n",
       " 'april',\n",
       " 'arab',\n",
       " 'archive',\n",
       " 'area',\n",
       " 'areas',\n",
       " 'aren',\n",
       " 'argument',\n",
       " 'arguments',\n",
       " 'armenia',\n",
       " 'armenian',\n",
       " 'armenians',\n",
       " 'arms',\n",
       " 'army',\n",
       " 'article',\n",
       " 'articles',\n",
       " 'asked',\n",
       " 'asking',\n",
       " 'assume',\n",
       " 'assuming',\n",
       " 'atheism',\n",
       " 'atheists',\n",
       " 'attack',\n",
       " 'attempt',\n",
       " 'author',\n",
       " 'authority',\n",
       " 'available',\n",
       " 'average',\n",
       " 'avoid',\n",
       " 'away',\n",
       " 'background',\n",
       " 'base',\n",
       " 'baseball',\n",
       " 'based',\n",
       " 'basic',\n",
       " 'basically',\n",
       " 'basis',\n",
       " 'begin',\n",
       " 'beginning',\n",
       " 'belief',\n",
       " 'beliefs',\n",
       " 'believe',\n",
       " 'best',\n",
       " 'better',\n",
       " 'bible',\n",
       " 'bike',\n",
       " 'bios',\n",
       " 'bits',\n",
       " 'black',\n",
       " 'block',\n",
       " 'blood',\n",
       " 'blue',\n",
       " 'board',\n",
       " 'body',\n",
       " 'book',\n",
       " 'books',\n",
       " 'boston',\n",
       " 'bought',\n",
       " 'break',\n",
       " 'bring',\n",
       " 'brought',\n",
       " 'build',\n",
       " 'building',\n",
       " 'built',\n",
       " 'business',\n",
       " 'cable',\n",
       " 'california',\n",
       " 'called',\n",
       " 'calling',\n",
       " 'calls',\n",
       " 'came',\n",
       " 'canada',\n",
       " 'card',\n",
       " 'cards',\n",
       " 'care',\n",
       " 'carry',\n",
       " 'cars',\n",
       " 'case',\n",
       " 'cases',\n",
       " 'cause',\n",
       " 'center',\n",
       " 'certain',\n",
       " 'certainly',\n",
       " 'chance',\n",
       " 'change',\n",
       " 'changed',\n",
       " 'changes',\n",
       " 'char',\n",
       " 'character',\n",
       " 'cheap',\n",
       " 'check',\n",
       " 'chicago',\n",
       " 'child',\n",
       " 'children',\n",
       " 'chip',\n",
       " 'chips',\n",
       " 'choice',\n",
       " 'christ',\n",
       " 'christian',\n",
       " 'christianity',\n",
       " 'christians',\n",
       " 'church',\n",
       " 'citizens',\n",
       " 'city',\n",
       " 'civil',\n",
       " 'claim',\n",
       " 'claims',\n",
       " 'class',\n",
       " 'clear',\n",
       " 'clearly',\n",
       " 'clinton',\n",
       " 'clipper',\n",
       " 'clock',\n",
       " 'close',\n",
       " 'code',\n",
       " 'color',\n",
       " 'colorado',\n",
       " 'colors',\n",
       " 'come',\n",
       " 'comes',\n",
       " 'coming',\n",
       " 'command',\n",
       " 'comment',\n",
       " 'comments',\n",
       " 'commercial',\n",
       " 'committee',\n",
       " 'common',\n",
       " 'communications',\n",
       " 'community',\n",
       " 'comp',\n",
       " 'companies',\n",
       " 'company',\n",
       " 'complete',\n",
       " 'completely',\n",
       " 'computer',\n",
       " 'conclusion',\n",
       " 'condition',\n",
       " 'conference',\n",
       " 'congress',\n",
       " 'consider',\n",
       " 'considered',\n",
       " 'considering',\n",
       " 'contact',\n",
       " 'contains',\n",
       " 'context',\n",
       " 'continue',\n",
       " 'contrib',\n",
       " 'control',\n",
       " 'controller',\n",
       " 'convert',\n",
       " 'copy',\n",
       " 'correct',\n",
       " 'cost',\n",
       " 'costs',\n",
       " 'couldn',\n",
       " 'count',\n",
       " 'countries',\n",
       " 'country',\n",
       " 'couple',\n",
       " 'course',\n",
       " 'court',\n",
       " 'cover',\n",
       " 'create',\n",
       " 'created',\n",
       " 'crime',\n",
       " 'criminals',\n",
       " 'cross',\n",
       " 'current',\n",
       " 'currently',\n",
       " 'data',\n",
       " 'date',\n",
       " 'dave',\n",
       " 'david',\n",
       " 'days',\n",
       " 'dead',\n",
       " 'deal',\n",
       " 'death',\n",
       " 'decided',\n",
       " 'decision',\n",
       " 'default',\n",
       " 'defense',\n",
       " 'define',\n",
       " 'deleted',\n",
       " 'department',\n",
       " 'described',\n",
       " 'description',\n",
       " 'design',\n",
       " 'designed',\n",
       " 'details',\n",
       " 'detroit',\n",
       " 'developed',\n",
       " 'development',\n",
       " 'device',\n",
       " 'devices',\n",
       " 'didn',\n",
       " 'died',\n",
       " 'difference',\n",
       " 'different',\n",
       " 'difficult',\n",
       " 'digital',\n",
       " 'direct',\n",
       " 'directly',\n",
       " 'directory',\n",
       " 'discussion',\n",
       " 'disease',\n",
       " 'disk',\n",
       " 'disks',\n",
       " 'display',\n",
       " 'distribution',\n",
       " 'division',\n",
       " 'doctor',\n",
       " 'does',\n",
       " 'doesn',\n",
       " 'doing',\n",
       " 'door',\n",
       " 'double',\n",
       " 'doubt',\n",
       " 'draw',\n",
       " 'drive',\n",
       " 'driver',\n",
       " 'drivers',\n",
       " 'drives',\n",
       " 'driving',\n",
       " 'drug',\n",
       " 'drugs',\n",
       " 'earlier',\n",
       " 'early',\n",
       " 'earth',\n",
       " 'easily',\n",
       " 'east',\n",
       " 'easy',\n",
       " 'education',\n",
       " 'effect',\n",
       " 'effective',\n",
       " 'effort',\n",
       " 'electronic',\n",
       " 'email',\n",
       " 'encryption',\n",
       " 'enforcement',\n",
       " 'engine',\n",
       " 'entire',\n",
       " 'entry',\n",
       " 'environment',\n",
       " 'equipment',\n",
       " 'error',\n",
       " 'errors',\n",
       " 'escrow',\n",
       " 'especially',\n",
       " 'event',\n",
       " 'events',\n",
       " 'evidence',\n",
       " 'exactly',\n",
       " 'example',\n",
       " 'excellent',\n",
       " 'exist',\n",
       " 'existence',\n",
       " 'exists',\n",
       " 'expect',\n",
       " 'expected',\n",
       " 'expensive',\n",
       " 'experience',\n",
       " 'explain',\n",
       " 'export',\n",
       " 'extra',\n",
       " 'face',\n",
       " 'fact',\n",
       " 'fairly',\n",
       " 'faith',\n",
       " 'fall',\n",
       " 'false',\n",
       " 'family',\n",
       " 'fast',\n",
       " 'faster',\n",
       " 'father',\n",
       " 'features',\n",
       " 'federal',\n",
       " 'feel',\n",
       " 'field',\n",
       " 'figure',\n",
       " 'file',\n",
       " 'files',\n",
       " 'final',\n",
       " 'finally',\n",
       " 'fine',\n",
       " 'firearms',\n",
       " 'floppy',\n",
       " 'folks',\n",
       " 'follow',\n",
       " 'following',\n",
       " 'font',\n",
       " 'fonts',\n",
       " 'food',\n",
       " 'force',\n",
       " 'forget',\n",
       " 'form',\n",
       " 'format',\n",
       " 'free',\n",
       " 'freedom',\n",
       " 'friend',\n",
       " 'friends',\n",
       " 'fully',\n",
       " 'function',\n",
       " 'functions',\n",
       " 'future',\n",
       " 'game',\n",
       " 'games',\n",
       " 'gave',\n",
       " 'general',\n",
       " 'generally',\n",
       " 'genocide',\n",
       " 'germany',\n",
       " 'gets',\n",
       " 'getting',\n",
       " 'given',\n",
       " 'gives',\n",
       " 'giving',\n",
       " 'goal',\n",
       " 'goals',\n",
       " 'goes',\n",
       " 'going',\n",
       " 'gone',\n",
       " 'good',\n",
       " 'government',\n",
       " 'graphics',\n",
       " 'great',\n",
       " 'greatly',\n",
       " 'greek',\n",
       " 'ground',\n",
       " 'group',\n",
       " 'groups',\n",
       " 'guess',\n",
       " 'guns',\n",
       " 'guys',\n",
       " 'half',\n",
       " 'hand',\n",
       " 'handle',\n",
       " 'hands',\n",
       " 'happen',\n",
       " 'happened',\n",
       " 'happens',\n",
       " 'happy',\n",
       " 'hard',\n",
       " 'hardware',\n",
       " 'haven',\n",
       " 'having',\n",
       " 'head',\n",
       " 'health',\n",
       " 'hear',\n",
       " 'heard',\n",
       " 'held',\n",
       " 'hell',\n",
       " 'hello',\n",
       " 'help',\n",
       " 'high',\n",
       " 'higher',\n",
       " 'history',\n",
       " 'hockey',\n",
       " 'hold',\n",
       " 'holy',\n",
       " 'home',\n",
       " 'hope',\n",
       " 'hours',\n",
       " 'house',\n",
       " 'human',\n",
       " 'idea',\n",
       " 'ideas',\n",
       " 'image',\n",
       " 'images',\n",
       " 'imagine',\n",
       " 'important',\n",
       " 'include',\n",
       " 'included',\n",
       " 'includes',\n",
       " 'including',\n",
       " 'increase',\n",
       " 'independent',\n",
       " 'individual',\n",
       " 'info',\n",
       " 'information',\n",
       " 'input',\n",
       " 'inside',\n",
       " 'installed',\n",
       " 'instead',\n",
       " 'institute',\n",
       " 'insurance',\n",
       " 'intended',\n",
       " 'interested',\n",
       " 'interesting',\n",
       " 'interface',\n",
       " 'internal',\n",
       " 'international',\n",
       " 'internet',\n",
       " 'involved',\n",
       " 'israel',\n",
       " 'israeli',\n",
       " 'issue',\n",
       " 'issues',\n",
       " 'james',\n",
       " 'jesus',\n",
       " 'jewish',\n",
       " 'jews',\n",
       " 'jobs',\n",
       " 'john',\n",
       " 'jpeg',\n",
       " 'just',\n",
       " 'kept',\n",
       " 'keyboard',\n",
       " 'keys',\n",
       " 'kill',\n",
       " 'killed',\n",
       " 'killing',\n",
       " 'kind',\n",
       " 'king',\n",
       " 'knew',\n",
       " 'know',\n",
       " 'knowledge',\n",
       " 'known',\n",
       " 'knows',\n",
       " 'lack',\n",
       " 'land',\n",
       " 'language',\n",
       " 'large',\n",
       " 'late',\n",
       " 'later',\n",
       " 'latest',\n",
       " 'launch',\n",
       " 'laws',\n",
       " 'lead',\n",
       " 'league',\n",
       " 'learn',\n",
       " 'leave',\n",
       " 'left',\n",
       " 'legal',\n",
       " 'letter',\n",
       " 'level',\n",
       " 'library',\n",
       " 'license',\n",
       " 'life',\n",
       " 'light',\n",
       " 'like',\n",
       " 'likely',\n",
       " 'limited',\n",
       " 'line',\n",
       " 'lines',\n",
       " 'list',\n",
       " 'little',\n",
       " 'live',\n",
       " 'lives',\n",
       " 'living',\n",
       " 'local',\n",
       " 'logic',\n",
       " 'long',\n",
       " 'longer',\n",
       " 'look',\n",
       " 'looked',\n",
       " 'looking',\n",
       " 'looks',\n",
       " 'lord',\n",
       " 'lost',\n",
       " 'lots',\n",
       " 'love',\n",
       " 'lower',\n",
       " 'lunar',\n",
       " 'machine',\n",
       " 'machines',\n",
       " 'magazine',\n",
       " 'mail',\n",
       " 'mailing',\n",
       " 'main',\n",
       " 'major',\n",
       " 'majority',\n",
       " 'make',\n",
       " 'makes',\n",
       " 'making',\n",
       " 'manager',\n",
       " 'manual',\n",
       " 'march',\n",
       " 'mark',\n",
       " 'market',\n",
       " 'mass',\n",
       " 'master',\n",
       " 'material',\n",
       " 'matter',\n",
       " 'matthew',\n",
       " 'maybe',\n",
       " 'mean',\n",
       " 'meaning',\n",
       " 'means',\n",
       " 'media',\n",
       " 'medical',\n",
       " 'member',\n",
       " 'members',\n",
       " 'memory',\n",
       " 'mention',\n",
       " 'mentioned',\n",
       " 'message',\n",
       " 'messages',\n",
       " 'method',\n",
       " 'michael',\n",
       " 'middle',\n",
       " 'mike',\n",
       " 'miles',\n",
       " 'military',\n",
       " 'million',\n",
       " 'mind',\n",
       " 'minutes',\n",
       " 'misc',\n",
       " 'mission',\n",
       " 'mode',\n",
       " 'model',\n",
       " 'models',\n",
       " 'modem',\n",
       " 'modern',\n",
       " 'money',\n",
       " 'monitor',\n",
       " 'month',\n",
       " 'months',\n",
       " 'moon',\n",
       " 'moral',\n",
       " 'mother',\n",
       " 'motif',\n",
       " 'mouse',\n",
       " 'multi',\n",
       " 'multiple',\n",
       " 'muslim',\n",
       " 'names',\n",
       " 'nasa',\n",
       " 'national',\n",
       " 'natural',\n",
       " 'nature',\n",
       " 'near',\n",
       " 'necessary',\n",
       " 'need',\n",
       " 'needed',\n",
       " 'needs',\n",
       " 'network',\n",
       " 'news',\n",
       " 'newsgroup',\n",
       " 'nice',\n",
       " 'night',\n",
       " 'normal',\n",
       " 'north',\n",
       " 'note',\n",
       " 'null',\n",
       " 'number',\n",
       " 'numbers',\n",
       " 'object',\n",
       " 'objective',\n",
       " 'obvious',\n",
       " 'obviously',\n",
       " 'offer',\n",
       " 'offers',\n",
       " 'office',\n",
       " 'official',\n",
       " 'ones',\n",
       " 'open',\n",
       " 'operation',\n",
       " 'opinion',\n",
       " 'opinions',\n",
       " 'option',\n",
       " 'options',\n",
       " 'orbit',\n",
       " 'order',\n",
       " 'organization',\n",
       " 'original',\n",
       " 'output',\n",
       " 'outside',\n",
       " 'package',\n",
       " 'page',\n",
       " 'paid',\n",
       " 'pain',\n",
       " 'paper',\n",
       " 'particular',\n",
       " 'particularly',\n",
       " 'parts',\n",
       " 'party',\n",
       " 'pass',\n",
       " 'passed',\n",
       " 'past',\n",
       " 'patients',\n",
       " 'paul',\n",
       " 'peace',\n",
       " 'people',\n",
       " 'perfect',\n",
       " 'performance',\n",
       " 'period',\n",
       " 'person',\n",
       " 'personal',\n",
       " 'peter',\n",
       " 'phone',\n",
       " 'physical',\n",
       " 'pick',\n",
       " 'picture',\n",
       " 'pittsburgh',\n",
       " 'place',\n",
       " 'places',\n",
       " 'plan',\n",
       " 'play',\n",
       " 'played',\n",
       " 'player',\n",
       " 'players',\n",
       " 'playing',\n",
       " 'plus',\n",
       " 'point',\n",
       " 'points',\n",
       " 'police',\n",
       " 'policy',\n",
       " 'political',\n",
       " 'poor',\n",
       " 'population',\n",
       " 'port',\n",
       " 'position',\n",
       " 'possible',\n",
       " 'possibly',\n",
       " 'post',\n",
       " 'posted',\n",
       " 'posting',\n",
       " 'posts',\n",
       " 'postscript',\n",
       " 'power',\n",
       " 'practice',\n",
       " 'present',\n",
       " 'president',\n",
       " 'press',\n",
       " 'pretty',\n",
       " 'prevent',\n",
       " 'previous',\n",
       " 'price',\n",
       " 'print',\n",
       " 'printer',\n",
       " 'privacy',\n",
       " 'private',\n",
       " 'probably',\n",
       " 'problem',\n",
       " 'problems',\n",
       " 'process',\n",
       " 'processing',\n",
       " 'produce',\n",
       " 'product',\n",
       " 'products',\n",
       " 'program',\n",
       " 'programming',\n",
       " 'programs',\n",
       " 'project',\n",
       " 'protect',\n",
       " 'protection',\n",
       " 'prove',\n",
       " 'provide',\n",
       " 'provided',\n",
       " 'provides',\n",
       " 'public',\n",
       " 'published',\n",
       " 'purpose',\n",
       " 'quality',\n",
       " 'question',\n",
       " 'questions',\n",
       " 'quite',\n",
       " 'quote',\n",
       " 'radio',\n",
       " 'range',\n",
       " 'rate',\n",
       " 'rates',\n",
       " 'read',\n",
       " 'reading',\n",
       " 'real',\n",
       " 'reality',\n",
       " 'realize',\n",
       " 'really',\n",
       " 'reason',\n",
       " 'reasonable',\n",
       " 'reasons',\n",
       " 'receive',\n",
       " 'received',\n",
       " 'recent',\n",
       " 'recently',\n",
       " 'record',\n",
       " 'reference',\n",
       " 'references',\n",
       " 'regarding',\n",
       " 'regular',\n",
       " 'related',\n",
       " 'release',\n",
       " 'religion',\n",
       " 'religious',\n",
       " 'remember',\n",
       " 'reply',\n",
       " 'report',\n",
       " 'reported',\n",
       " 'reports',\n",
       " 'request',\n",
       " 'require',\n",
       " 'required',\n",
       " 'requires',\n",
       " 'research',\n",
       " 'resource',\n",
       " 'resources',\n",
       " 'respect',\n",
       " 'response',\n",
       " 'rest',\n",
       " 'result',\n",
       " 'results',\n",
       " 'return',\n",
       " 'right',\n",
       " 'rights',\n",
       " 'risk',\n",
       " 'road',\n",
       " 'robert',\n",
       " 'room',\n",
       " 'round',\n",
       " 'rule',\n",
       " 'rules',\n",
       " 'running',\n",
       " 'runs',\n",
       " 'russia',\n",
       " 'russian',\n",
       " 'safe',\n",
       " 'safety',\n",
       " 'said',\n",
       " 'sale',\n",
       " 'satellite',\n",
       " 'save',\n",
       " 'saying',\n",
       " 'says',\n",
       " 'school',\n",
       " 'science',\n",
       " 'scientific',\n",
       " 'screen',\n",
       " 'scsi',\n",
       " 'search',\n",
       " 'season',\n",
       " 'second',\n",
       " 'secret',\n",
       " 'section',\n",
       " 'secure',\n",
       " 'security',\n",
       " 'seen',\n",
       " 'self',\n",
       " 'sell',\n",
       " 'send',\n",
       " 'sense',\n",
       " 'sent',\n",
       " 'serial',\n",
       " 'series',\n",
       " 'seriously',\n",
       " 'server',\n",
       " 'service',\n",
       " 'services',\n",
       " 'setting',\n",
       " 'shall',\n",
       " 'shipping',\n",
       " 'short',\n",
       " 'shot',\n",
       " 'shouldn',\n",
       " 'shows',\n",
       " 'shuttle',\n",
       " 'similar',\n",
       " 'simple',\n",
       " 'simply',\n",
       " 'single',\n",
       " 'site',\n",
       " 'sites',\n",
       " 'situation',\n",
       " 'size',\n",
       " 'slow',\n",
       " 'small',\n",
       " 'smith',\n",
       " 'social',\n",
       " 'society',\n",
       " 'software',\n",
       " 'sold',\n",
       " 'soldiers',\n",
       " 'solution',\n",
       " 'somebody',\n",
       " 'soon',\n",
       " 'sorry',\n",
       " 'sort',\n",
       " 'sound',\n",
       " 'sounds',\n",
       " 'source',\n",
       " 'sources',\n",
       " 'south',\n",
       " 'soviet',\n",
       " 'space',\n",
       " 'speak',\n",
       " 'special',\n",
       " 'specific',\n",
       " 'specifically',\n",
       " 'speed',\n",
       " 'spirit',\n",
       " 'stand',\n",
       " 'standard',\n",
       " 'standards',\n",
       " 'start',\n",
       " 'started',\n",
       " 'starting',\n",
       " 'state',\n",
       " 'stated',\n",
       " 'statement',\n",
       " 'states',\n",
       " 'station',\n",
       " 'stay',\n",
       " 'step',\n",
       " 'stephanopoulos',\n",
       " 'steve',\n",
       " 'stop',\n",
       " 'story',\n",
       " 'stream',\n",
       " 'street',\n",
       " 'strong',\n",
       " 'studies',\n",
       " 'study',\n",
       " 'stuff',\n",
       " 'stupid',\n",
       " 'subject',\n",
       " 'suggest',\n",
       " 'suggestions',\n",
       " 'summer',\n",
       " 'supply',\n",
       " 'support',\n",
       " 'supported',\n",
       " 'supports',\n",
       " 'suppose',\n",
       " 'supposed',\n",
       " 'sure',\n",
       " 'surface',\n",
       " 'suspect',\n",
       " 'switch',\n",
       " 'systems',\n",
       " 'table',\n",
       " 'taken',\n",
       " 'takes',\n",
       " 'taking',\n",
       " 'talk',\n",
       " 'talking',\n",
       " 'tape',\n",
       " 'team',\n",
       " 'teams',\n",
       " 'technical',\n",
       " 'technology',\n",
       " 'tell',\n",
       " 'term',\n",
       " 'terms',\n",
       " 'test',\n",
       " 'text',\n",
       " 'thank',\n",
       " 'thanks',\n",
       " 'theory',\n",
       " 'thing',\n",
       " 'things',\n",
       " 'think',\n",
       " 'thinking',\n",
       " 'thought',\n",
       " 'time',\n",
       " 'times',\n",
       " 'title',\n",
       " 'today',\n",
       " 'told',\n",
       " 'took',\n",
       " 'tools',\n",
       " 'toronto',\n",
       " 'total',\n",
       " 'town',\n",
       " 'trade',\n",
       " 'traffic',\n",
       " 'transfer',\n",
       " 'tried',\n",
       " 'trouble',\n",
       " 'true',\n",
       " 'trust',\n",
       " 'truth',\n",
       " 'trying',\n",
       " 'turkey',\n",
       " 'turkish',\n",
       " 'turks',\n",
       " 'turn',\n",
       " 'turned',\n",
       " 'type',\n",
       " 'types',\n",
       " 'understand',\n",
       " 'understanding',\n",
       " 'unfortunately',\n",
       " 'unit',\n",
       " 'united',\n",
       " 'universe',\n",
       " 'university',\n",
       " 'unix',\n",
       " 'unless',\n",
       " 'used',\n",
       " 'useful',\n",
       " 'usenet',\n",
       " 'user',\n",
       " 'users',\n",
       " 'uses',\n",
       " 'using',\n",
       " 'usually',\n",
       " 'value',\n",
       " 'values',\n",
       " 'vancouver',\n",
       " 'various',\n",
       " 'vehicle',\n",
       " 'version',\n",
       " 'versions',\n",
       " 'video',\n",
       " 'view',\n",
       " 'voice',\n",
       " 'volume',\n",
       " 'wait',\n",
       " 'want',\n",
       " 'wanted',\n",
       " 'wants',\n",
       " 'washington',\n",
       " 'wasn',\n",
       " 'watch',\n",
       " 'water',\n",
       " 'ways',\n",
       " 'weapon',\n",
       " 'weapons',\n",
       " 'week',\n",
       " 'weeks',\n",
       " 'went',\n",
       " 'west',\n",
       " 'western',\n",
       " 'white',\n",
       " 'wide',\n",
       " 'widget',\n",
       " 'wife',\n",
       " 'willing',\n",
       " 'window',\n",
       " 'windows',\n",
       " 'wire',\n",
       " 'wish',\n",
       " 'woman',\n",
       " 'women',\n",
       " 'wonder',\n",
       " 'wondering',\n",
       " 'word',\n",
       " 'words',\n",
       " 'work',\n",
       " 'worked',\n",
       " 'working',\n",
       " 'works',\n",
       " 'world',\n",
       " 'worse',\n",
       " 'worth',\n",
       " 'wouldn',\n",
       " 'write',\n",
       " 'writing',\n",
       " 'written',\n",
       " 'wrong',\n",
       " 'wrote',\n",
       " 'xterm',\n",
       " 'yeah',\n",
       " 'year',\n",
       " 'years',\n",
       " 'york',\n",
       " 'young']"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "terms=vector.get_feature_names()#1000개의 단어\n",
    "terms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "토픽 1: [('just', 0.20887), ('like', 0.20469), ('know', 0.19349), ('people', 0.18318), ('think', 0.1697), ('does', 0.15336), ('good', 0.1438), ('time', 0.13656), ('thanks', 0.11063), ('make', 0.10461)]\n",
      "토픽 2: [('thanks', 0.32759), ('windows', 0.28791), ('card', 0.18011), ('drive', 0.16867), ('mail', 0.15263), ('file', 0.14387), ('advance', 0.12512), ('files', 0.11334), ('software', 0.11272), ('does', 0.11075)]\n",
      "토픽 3: [('game', 0.34024), ('team', 0.30231), ('year', 0.27044), ('games', 0.23506), ('drive', 0.17202), ('season', 0.17072), ('good', 0.15077), ('players', 0.14866), ('play', 0.14243), ('hockey', 0.12633)]\n",
      "토픽 4: [('drive', 0.46219), ('scsi', 0.17163), ('disk', 0.14324), ('hard', 0.13746), ('problem', 0.12619), ('just', 0.11944), ('drives', 0.1194), ('card', 0.11135), ('controller', 0.08503), ('floppy', 0.08267)]\n",
      "토픽 5: [('drive', 0.40148), ('know', 0.29064), ('thanks', 0.24761), ('does', 0.24387), ('just', 0.18098), ('scsi', 0.1523), ('drives', 0.10927), ('hard', 0.09927), ('controller', 0.07737), ('like', 0.07563)]\n",
      "토픽 6: [('just', 0.55569), ('like', 0.23144), ('windows', 0.22918), ('know', 0.16081), ('does', 0.11132), ('file', 0.10334), ('window', 0.10021), ('think', 0.09621), ('files', 0.07958), ('problem', 0.06761)]\n",
      "토픽 7: [('just', 0.42987), ('like', 0.23402), ('mail', 0.14697), ('bike', 0.11382), ('thanks', 0.10347), ('email', 0.0988), ('chip', 0.09571), ('space', 0.09454), ('sale', 0.08571), ('send', 0.07971)]\n",
      "토픽 8: [('does', 0.38381), ('know', 0.26671), ('chip', 0.21525), ('like', 0.19108), ('card', 0.13203), ('clipper', 0.12533), ('encryption', 0.12086), ('government', 0.11004), ('keys', 0.09355), ('data', 0.08228)]\n",
      "토픽 9: [('like', 0.40944), ('card', 0.36266), ('sale', 0.21341), ('video', 0.16815), ('offer', 0.14968), ('jesus', 0.13784), ('price', 0.13363), ('monitor', 0.12895), ('condition', 0.1258), ('shipping', 0.10759)]\n",
      "토픽 10: [('like', 0.61708), ('drive', 0.24746), ('file', 0.16773), ('files', 0.10778), ('think', 0.08888), ('sounds', 0.08669), ('program', 0.0735), ('space', 0.06878), ('look', 0.06294), ('drives', 0.06217)]\n",
      "토픽 11: [('people', 0.42263), ('like', 0.22265), ('government', 0.19204), ('thanks', 0.19016), ('card', 0.18494), ('windows', 0.14639), ('armenian', 0.1205), ('armenians', 0.11172), ('right', 0.10847), ('game', 0.09402)]\n",
      "토픽 12: [('think', 0.78394), ('chip', 0.13547), ('mail', 0.12651), ('need', 0.11258), ('clipper', 0.09434), ('encryption', 0.09271), ('address', 0.08225), ('data', 0.07038), ('keys', 0.06193), ('send', 0.05667)]\n",
      "토픽 13: [('does', 0.2753), ('just', 0.26047), ('like', 0.21629), ('game', 0.20185), ('jesus', 0.15145), ('file', 0.14982), ('team', 0.1162), ('games', 0.11353), ('list', 0.10046), ('scsi', 0.099)]\n",
      "토픽 14: [('know', 0.45225), ('people', 0.25188), ('windows', 0.22803), ('good', 0.21834), ('sale', 0.14528), ('file', 0.13795), ('files', 0.12878), ('price', 0.11898), ('offer', 0.08746), ('year', 0.08351)]\n",
      "토픽 15: [('space', 0.47993), ('know', 0.19242), ('nasa', 0.18287), ('card', 0.14581), ('time', 0.12929), ('article', 0.1128), ('think', 0.10387), ('shuttle', 0.09212), ('launch', 0.08645), ('didn', 0.08206)]\n",
      "토픽 16: [('does', 0.3181), ('window', 0.26247), ('right', 0.25954), ('think', 0.21183), ('sale', 0.21111), ('problem', 0.14607), ('offer', 0.1366), ('bike', 0.12831), ('condition', 0.12299), ('price', 0.12062)]\n",
      "토픽 17: [('good', 0.38682), ('israel', 0.31074), ('does', 0.28075), ('israeli', 0.16264), ('space', 0.12625), ('windows', 0.12476), ('jews', 0.12173), ('looking', 0.10652), ('arab', 0.09276), ('armenian', 0.09267)]\n",
      "토픽 18: [('right', 0.38256), ('file', 0.24863), ('windows', 0.18032), ('bike', 0.17425), ('time', 0.12716), ('looking', 0.12525), ('card', 0.11277), ('said', 0.1088), ('believe', 0.10416), ('left', 0.08619)]\n",
      "토픽 19: [('file', 0.28384), ('problem', 0.24019), ('israel', 0.23375), ('need', 0.19515), ('know', 0.17931), ('time', 0.13943), ('jews', 0.129), ('israeli', 0.1241), ('think', 0.1224), ('email', 0.11459)]\n",
      "토픽 20: [('need', 0.31785), ('file', 0.29075), ('make', 0.20196), ('right', 0.16651), ('people', 0.14525), ('mail', 0.13603), ('card', 0.12823), ('work', 0.1217), ('want', 0.10941), ('help', 0.10237)]\n"
     ]
    }
   ],
   "source": [
    "def getTopic(c, fName, n=10):\n",
    "    for i, t in enumerate(c):\n",
    "        print(\"토픽 %d:\"% (i+1),[(fName[i],t[i].round(5)) for i in t.argsort()[:-n-1:-1]])\n",
    "getTopic(svdModel.components_,terms)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "low : 5\n",
    "lower : 2\n",
    "newest : 6\n",
    "widest : 3\n",
    "\"\"\"\n",
    "# l,o,w,e,r,n,s,t,d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3\n",
      "2\n",
      "1\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "7"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
