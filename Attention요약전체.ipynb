{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import re\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from bs4 import BeautifulSoup\n",
    "from nltk.corpus import stopwords\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100000\n"
     ]
    }
   ],
   "source": [
    "data=pd.read_csv(\"C:/Users/student/Downloads/Python_JP/word2vec/Reviews.csv\", nrows=100000)\n",
    "print(len(data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Text</th>\n",
       "      <th>Summary</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>I have bought several of the Vitality canned d...</td>\n",
       "      <td>Good Quality Dog Food</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>Product arrived labeled as Jumbo Salted Peanut...</td>\n",
       "      <td>Not as Advertised</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>This is a confection that has been around a fe...</td>\n",
       "      <td>\"Delight\" says it all</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>If you are looking for the secret ingredient i...</td>\n",
       "      <td>Cough Medicine</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>Great taffy at a great price.  There was a wid...</td>\n",
       "      <td>Great taffy</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                Text                Summary\n",
       "0  I have bought several of the Vitality canned d...  Good Quality Dog Food\n",
       "1  Product arrived labeled as Jumbo Salted Peanut...      Not as Advertised\n",
       "2  This is a confection that has been around a fe...  \"Delight\" says it all\n",
       "3  If you are looking for the secret ingredient i...         Cough Medicine\n",
       "4  Great taffy at a great price.  There was a wid...            Great taffy"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#data.head(5)\n",
    "#data.info()\n",
    "data=data[['Text','Summary']]\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "88426"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#data['Text']   #text열에 대해 중복 제외한 데이터 개수 출력\n",
    "data['Text'].nunique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.drop_duplicates(subset=['Text'], \n",
    "                     inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "88426"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Text       0\n",
       "Summary    1\n",
       "dtype: int64"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.dropna(axis=0, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#전처리\n",
    "contractions = { \n",
    "\"ain't\": \"am not / are not / is not / has not / have not\",\n",
    "\"aren't\": \"are not / am not\",\n",
    "\"can't\": \"cannot\",\n",
    "\"can't've\": \"cannot have\",\n",
    "\"'cause\": \"because\",\n",
    "\"could've\": \"could have\",\n",
    "\"couldn't\": \"could not\",\n",
    "\"couldn't've\": \"could not have\",\n",
    "\"didn't\": \"did not\",\n",
    "\"doesn't\": \"does not\",\n",
    "\"don't\": \"do not\",\n",
    "\"hadn't\": \"had not\",\n",
    "\"hadn't've\": \"had not have\",\n",
    "\"hasn't\": \"has not\",\n",
    "\"haven't\": \"have not\",\n",
    "\"he'd\": \"he had / he would\",\n",
    "\"he'd've\": \"he would have\",\n",
    "\"he'll\": \"he shall / he will\",\n",
    "\"he'll've\": \"he shall have / he will have\",\n",
    "\"he's\": \"he has / he is\",\n",
    "\"how'd\": \"how did\",\n",
    "\"how'd'y\": \"how do you\",\n",
    "\"how'll\": \"how will\",\n",
    "\"how's\": \"how has / how is / how does\",\n",
    "\"I'd\": \"I had / I would\",\n",
    "\"I'd've\": \"I would have\",\n",
    "\"I'll\": \"I shall / I will\",\n",
    "\"I'll've\": \"I shall have / I will have\",\n",
    "\"I'm\": \"I am\",\n",
    "\"I've\": \"I have\",\n",
    "\"isn't\": \"is not\",\n",
    "\"it'd\": \"it had / it would\",\n",
    "\"it'd've\": \"it would have\",\n",
    "\"it'll\": \"it shall / it will\",\n",
    "\"it'll've\": \"it shall have / it will have\",\n",
    "\"it's\": \"it has / it is\",\n",
    "\"let's\": \"let us\",\n",
    "\"ma'am\": \"madam\",\n",
    "\"mayn't\": \"may not\",\n",
    "\"might've\": \"might have\",\n",
    "\"mightn't\": \"might not\",\n",
    "\"mightn't've\": \"might not have\",\n",
    "\"must've\": \"must have\",\n",
    "\"mustn't\": \"must not\",\n",
    "\"mustn't've\": \"must not have\",\n",
    "\"needn't\": \"need not\",\n",
    "\"needn't've\": \"need not have\",\n",
    "\"o'clock\": \"of the clock\",\n",
    "\"oughtn't\": \"ought not\",\n",
    "\"oughtn't've\": \"ought not have\",\n",
    "\"shan't\": \"shall not\",\n",
    "\"sha'n't\": \"shall not\",\n",
    "\"shan't've\": \"shall not have\",\n",
    "\"she'd\": \"she had / she would\",\n",
    "\"she'd've\": \"she would have\",\n",
    "\"she'll\": \"she shall / she will\",\n",
    "\"she'll've\": \"she shall have / she will have\",\n",
    "\"she's\": \"she has / she is\",\n",
    "\"should've\": \"should have\",\n",
    "\"shouldn't\": \"should not\",\n",
    "\"shouldn't've\": \"should not have\",\n",
    "\"so've\": \"so have\",\n",
    "\"so's\": \"so as / so is\",\n",
    "\"that'd\": \"that would / that had\",\n",
    "\"that'd've\": \"that would have\",\n",
    "\"that's\": \"that has / that is\",\n",
    "\"there'd\": \"there had / there would\",\n",
    "\"there'd've\": \"there would have\",\n",
    "\"there's\": \"there has / there is\",\n",
    "\"they'd\": \"they had / they would\",\n",
    "\"they'd've\": \"they would have\",\n",
    "\"they'll\": \"they shall / they will\",\n",
    "\"they'll've\": \"they shall have / they will have\",\n",
    "\"they're\": \"they are\",\n",
    "\"they've\": \"they have\",\n",
    "\"to've\": \"to have\",\n",
    "\"wasn't\": \"was not\",\n",
    "\"we'd\": \"we had / we would\",\n",
    "\"we'd've\": \"we would have\",\n",
    "\"we'll\": \"we will\",\n",
    "\"we'll've\": \"we will have\",\n",
    "\"we're\": \"we are\",\n",
    "\"we've\": \"we have\",\n",
    "\"weren't\": \"were not\",\n",
    "\"what'll\": \"what shall / what will\",\n",
    "\"what'll've\": \"what shall have / what will have\",\n",
    "\"what're\": \"what are\",\n",
    "\"what's\": \"what has / what is\",\n",
    "\"what've\": \"what have\",\n",
    "\"when's\": \"when has / when is\",\n",
    "\"when've\": \"when have\",\n",
    "\"where'd\": \"where did\",\n",
    "\"where's\": \"where has / where is\",\n",
    "\"where've\": \"where have\",\n",
    "\"who'll\": \"who shall / who will\",\n",
    "\"who'll've\": \"who shall have / who will have\",\n",
    "\"who's\": \"who has / who is\",\n",
    "\"who've\": \"who have\",\n",
    "\"why's\": \"why has / why is\",\n",
    "\"why've\": \"why have\",\n",
    "\"will've\": \"will have\",\n",
    "\"won't\": \"will not\",\n",
    "\"won't've\": \"will not have\",\n",
    "\"would've\": \"would have\",\n",
    "\"wouldn't\": \"would not\",\n",
    "\"wouldn't've\": \"would not have\",\n",
    "\"y'all\": \"you all\",\n",
    "\"y'all'd\": \"you all would\",\n",
    "\"y'all'd've\": \"you all would have\",\n",
    "\"y'all're\": \"you all are\",\n",
    "\"y'all've\": \"you all have\",\n",
    "\"you'd\": \"you had / you would\",\n",
    "\"you'd've\": \"you would have\",\n",
    "\"you'll\": \"you shall / you will\",\n",
    "\"you'll've\": \"you shall have / you will have\",\n",
    "\"you're\": \"you are\",\n",
    "\"you've\": \"you have\"\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "179"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stopWords=set(stopwords.words('english'))\n",
    "len(stopWords) #179"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocessSentence(sent, rs=True):\n",
    "    sent=sent.lower()\n",
    "    sent=BeautifulSoup(sent, \"lxml\").text#html태그제거\n",
    "    sent=re.sub(\"\\([^)]*\\)\", \"\" , sent)\n",
    "    sent=re.sub('\"', \"\" , sent)\n",
    "    sent=\" \".join([contractions[t] if t in \n",
    "                   contractions else t for t in \n",
    "                   sent.split(\" \")])\n",
    "    sent=re.sub(\"'s\\b\", \"\",sent) #소유격제거\n",
    "    sent=re.sub(\"[^a-zA-Z]\", \" \",sent)\n",
    "    sent=re.sub(\"[m]{2,}\", \"mm\", sent)\n",
    "    \n",
    "    #rs==True => 불용어 제거(원문)\n",
    "    if rs:\n",
    "        tokens=\" \".join(word for word in sent.split() \n",
    "        if not word in stopWords \n",
    "        if len(word)>1)\n",
    "    else: #불용어 제거 안함(요약)\n",
    "        tokens=\" \".join(word for word in \n",
    "                        sent.split() if len(word)>1)    \n",
    "    return tokens    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "amm student\n"
     ]
    }
   ],
   "source": [
    "print(preprocessSentence(\"<a>I ammmmmmmmmm a student</a>\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['bought several vitality canned dog food products found good quality product looks like stew processed meat smells better labrador finicky appreciates product better', 'product arrived labeled jumbo salted peanuts peanuts actually small sized unsalted sure error vendor intended represent product jumbo', 'confection around centuries light pillowy citrus gelatin nuts case filberts cut tiny squares liberally coated powdered sugar tiny mouthful heaven chewy flavorful highly recommend yummy treat familiar story lewis lion witch wardrobe treat seduces edmund selling brother sisters witch', 'looking secret ingredient robitussin believe found got addition root beer extract ordered made cherry soda flavor medicinal', 'great taffy great price wide assortment yummy taffy delivery quick taffy lover deal', 'got wild hair taffy ordered five pound bag taffy enjoyable many flavors watermelon root beer melon peppermint grape etc complaint bit much red black licorice flavored pieces kids husband lasted two weeks would recommend brand taffy delightful treat', 'saltwater taffy great flavors soft chewy candy individually wrapped well none candies stuck together happen expensive version fralinger would highly recommend candy served beach themed party everyone loved', 'taffy good soft chewy flavors amazing would definitely recommend buying satisfying', 'right mostly sprouting cats eat grass love rotate around wheatgrass rye', 'healthy dog food good digestion also good small puppies dog eats required amount every feeding']\n"
     ]
    }
   ],
   "source": [
    "cleanText=[]\n",
    "for sent in data['Text']:\n",
    "    cleanText.append(preprocessSentence(sent))\n",
    "print(cleanText[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['Text']=cleanText"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\student\\Anaconda3\\lib\\site-packages\\bs4\\__init__.py:219: UserWarning: \"b'...'\" looks like a filename, not markup. You should probably open this file and pass the filehandle into Beautiful Soup.\n",
      "  ' Beautiful Soup.' % markup)\n",
      "C:\\Users\\student\\Anaconda3\\lib\\site-packages\\bs4\\__init__.py:282: UserWarning: \"http://www.amazon.com/gp/product/b007i7yygy/ref=cm_cr_rev_prod_title\" looks like a URL. Beautiful Soup is not an HTTP client. You should probably use an HTTP client like requests to get the document behind the URL, and feed that document to Beautiful Soup.\n",
      "  ' that document to Beautiful Soup.' % decoded_markup\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['good quality dog food', 'advertised', 'delight says', 'cough medicine', 'great taffy', 'nice taffy', 'great good expensive brands', 'wonderful tasty taffy', 'yay barley', 'healthy dog food']\n"
     ]
    }
   ],
   "source": [
    "cleanSummary=[]\n",
    "for sent in data['Summary']:\n",
    "    cleanSummary.append(preprocessSentence(sent))\n",
    "print(cleanSummary[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['Summary']=cleanSummary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Text</th>\n",
       "      <th>Summary</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>bought several vitality canned dog food produc...</td>\n",
       "      <td>good quality dog food</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>product arrived labeled jumbo salted peanuts p...</td>\n",
       "      <td>advertised</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>confection around centuries light pillowy citr...</td>\n",
       "      <td>delight says</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>looking secret ingredient robitussin believe f...</td>\n",
       "      <td>cough medicine</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>great taffy great price wide assortment yummy ...</td>\n",
       "      <td>great taffy</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>got wild hair taffy ordered five pound bag taf...</td>\n",
       "      <td>nice taffy</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>saltwater taffy great flavors soft chewy candy...</td>\n",
       "      <td>great good expensive brands</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>taffy good soft chewy flavors amazing would de...</td>\n",
       "      <td>wonderful tasty taffy</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>right mostly sprouting cats eat grass love rot...</td>\n",
       "      <td>yay barley</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>healthy dog food good digestion also good smal...</td>\n",
       "      <td>healthy dog food</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                Text  \\\n",
       "0  bought several vitality canned dog food produc...   \n",
       "1  product arrived labeled jumbo salted peanuts p...   \n",
       "2  confection around centuries light pillowy citr...   \n",
       "3  looking secret ingredient robitussin believe f...   \n",
       "4  great taffy great price wide assortment yummy ...   \n",
       "5  got wild hair taffy ordered five pound bag taf...   \n",
       "6  saltwater taffy great flavors soft chewy candy...   \n",
       "7  taffy good soft chewy flavors amazing would de...   \n",
       "8  right mostly sprouting cats eat grass love rot...   \n",
       "9  healthy dog food good digestion also good smal...   \n",
       "\n",
       "                       Summary  \n",
       "0        good quality dog food  \n",
       "1                   advertised  \n",
       "2                 delight says  \n",
       "3               cough medicine  \n",
       "4                  great taffy  \n",
       "5                   nice taffy  \n",
       "6  great good expensive brands  \n",
       "7        wonderful tasty taffy  \n",
       "8                   yay barley  \n",
       "9             healthy dog food  "
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.replace(\"\", np.nan,inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.isnull().sum()\n",
    "data.dropna(axis=0, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "88134"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "textLen=[len(s.split()) for s in data['Text']]\n",
    "summaryLen=[len(s.split()) for s in data['Summary']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2.872115188236095"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.min(textLen)#2\n",
    "np.max(textLen)#1235\n",
    "np.mean(textLen)#38\n",
    "np.min(summaryLen)#1\n",
    "np.max(summaryLen)#16\n",
    "np.mean(summaryLen)#2.8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "textMaxLen=50\n",
    "summaryMaxLen=8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def threshLen(mlen, nlist):\n",
    "    c=0\n",
    "    for s in nlist:\n",
    "        if(len(s.split()) <= mlen):\n",
    "            c+=1\n",
    "    print(c/len(nlist))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7746726575441941\n",
      "0.9943948986770146\n"
     ]
    }
   ],
   "source": [
    "threshLen(textMaxLen, data['Text'])\n",
    "threshLen(summaryMaxLen, data['Summary'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "data=data[data['Text'].apply(\n",
    "    lambda x:len(x.split())<=textMaxLen)]\n",
    "data=data[data['Summary'].apply(\n",
    "    lambda x:len(x.split())<=summaryMaxLen)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "68061\n"
     ]
    }
   ],
   "source": [
    "print(len(data)) #68천"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0        sostoken good quality dog food eostoken\n",
       "1                   sostoken advertised eostoken\n",
       "2                 sostoken delight says eostoken\n",
       "3               sostoken cough medicine eostoken\n",
       "4                  sostoken great taffy eostoken\n",
       "                          ...                   \n",
       "99993              sostoken great stuff eostoken\n",
       "99994               sostoken good stuff eostoken\n",
       "99995                    sostoken yummy eostoken\n",
       "99997              sostoken great ramen eostoken\n",
       "99998                    sostoken spicy eostoken\n",
       "Name: Summary, Length: 68061, dtype: object"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#seq2seq\n",
    "data['Summary']=data['Summary'].apply(lambda x:\"sostoken \"+  x  +\" eostoken\")\n",
    "data['Summary']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "textData=list(data['Text'])\n",
    "summaryData=list(data['Summary'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "xTrain, xTest, yTrain, yTest=train_test_split(textData, summaryData, test_size=0.2, random_state=42, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "13613"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(xTrain)#54448\n",
    "len(xTest)#13613"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "srcToken=Tokenizer()\n",
    "srcToken.fit_on_texts(xTrain)\n",
    "#단어 집합 생성"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "totalCnt=len(srcToken.word_index) #32000 여개"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "단어 빈도수 총합 1323945\n",
      "단어수 32668\n",
      "0.7870699155136525\n",
      "0.04192696826529803\n",
      "6956\n"
     ]
    }
   ],
   "source": [
    "totalFreq=0 #전체 단어 빈도수 총 합\n",
    "rCnt=0 #빈도수가 10 미만인 단어의 개수\n",
    "rFreq=0 #빈도수가 10 미만인 단어 빈도수 총 합\n",
    "for k, v in srcToken.word_counts.items():\n",
    "    totalFreq+=v\n",
    "    if(v<10):\n",
    "        rCnt+=1\n",
    "        rFreq+=v\n",
    "print(\"단어 빈도수 총합\", totalFreq)#1323945\n",
    "print(\"단어수\", totalCnt)#32688\n",
    "print(rCnt/totalCnt) #78%\n",
    "print(rFreq/totalFreq)#4%\n",
    "print(totalCnt-rCnt)  #6956 -> 7000\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "srcVocab=7000\n",
    "srcToken=Tokenizer(num_words=srcVocab)\n",
    "srcToken.fit_on_texts(xTrain)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "xTrain=srcToken.texts_to_sequences(xTrain)\n",
    "xTest=srcToken.texts_to_sequences(xTest)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[105, 170, 14, 491, 523, 7, 71, 1106, 45, 249, 1705, 170, 51, 33, 534, 1479, 718, 143, 168, 160, 135]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'bought several vitality canned dog food products found good quality product looks like stew processed meat smells better labrador finicky appreciates product better'"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(xTrain[0])\n",
    "data['Text'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "tarToken=Tokenizer()\n",
    "tarToken.fit_on_texts(yTrain)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "단어 빈도수 총합 254334\n",
      "단어수 32668\n",
      "0.28223337822946004\n",
      "0.07756336156392775\n",
      "23448\n"
     ]
    }
   ],
   "source": [
    "totalFreq=0 #전체 단어 빈도수 총 합\n",
    "rCnt=0 #빈도수가 10 미만인 단어의 개수\n",
    "rFreq=0 #빈도수가 10 미만인 단어 빈도수 총 합\n",
    "\n",
    "for k, v in tarToken.word_counts.items():\n",
    "    totalFreq+=v\n",
    "    if(v<10):\n",
    "        rCnt+=1\n",
    "        rFreq+=v\n",
    "        \n",
    "        \n",
    "print(\"단어 빈도수 총합\", totalFreq)#1323945\n",
    "print(\"단어수\", totalCnt)#32688\n",
    "print(rCnt/totalCnt) #78%\n",
    "print(rFreq/totalFreq)#4%\n",
    "print(totalCnt-rCnt)  #6956 -> 7000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "tarVoc=2000\n",
    "tarTokenizer=Tokenizer(num_words=tarVoc)\n",
    "tarTokenizer.fit_on_texts(yTrain)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "yTrain=tarTokenizer.texts_to_sequences(yTrain)\n",
    "yTest=tarTokenizer.texts_to_sequences(yTest)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[1, 44, 5, 2],\n",
       " [1, 16, 2],\n",
       " [1, 10, 735, 8, 2],\n",
       " [1, 33, 801, 85, 1300, 2],\n",
       " [1, 187, 234, 183, 54, 31, 2]]"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "yTrain[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "dropTrain=[i for i, sent in \n",
    "           enumerate(yTrain) \n",
    "           if len(sent)==2]\n",
    "dropTest=[i for i, sent in \n",
    "          enumerate(yTest) \n",
    "          if len(sent)==2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "xTrain=np.delete(xTrain, dropTrain, axis=0)\n",
    "yTrain=np.delete(yTrain, dropTrain, axis=0)\n",
    "xTest=np.delete(xTest, dropTest, axis=0)\n",
    "yTest=np.delete(yTest, dropTest, axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "13181"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(xTrain) #52722\n",
    "len(xTest) #13181"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "xTrain=pad_sequences(xTrain,maxlen=textMaxLen, padding='post')\n",
    "xTest=pad_sequences(xTest,maxlen=textMaxLen, padding='post')\n",
    "yTrain=pad_sequences(yTrain,maxlen=summaryMaxLen, padding='post')\n",
    "yTest=pad_sequences(yTest,maxlen=summaryMaxLen, padding='post')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\student\\Anaconda3\\lib\\site-packages\\tensorflow_core\\python\\keras\\initializers.py:119: calling RandomUniform.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Call initializer instance with the dtype argument instead of passing it to the constructor\n",
      "WARNING:tensorflow:From C:\\Users\\student\\Anaconda3\\lib\\site-packages\\tensorflow_core\\python\\ops\\resource_variable_ops.py:1630: calling BaseResourceVariable.__init__ (from tensorflow.python.ops.resource_variable_ops) with constraint is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "If using Keras pass *_constraint arguments to layers.\n",
      "Model: \"model\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_1 (InputLayer)            [(None, 50)]         0                                            \n",
      "__________________________________________________________________________________________________\n",
      "embedding (Embedding)           (None, 50, 128)      896000      input_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "lstm (LSTM)                     [(None, 50, 256), (N 394240      embedding[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "input_2 (InputLayer)            [(None, None)]       0                                            \n",
      "__________________________________________________________________________________________________\n",
      "lstm_1 (LSTM)                   [(None, 50, 256), (N 525312      lstm[0][0]                       \n",
      "__________________________________________________________________________________________________\n",
      "embedding_1 (Embedding)         (None, None, 128)    256000      input_2[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "lstm_2 (LSTM)                   [(None, 50, 256), (N 525312      lstm_1[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "lstm_3 (LSTM)                   [(None, None, 256),  394240      embedding_1[0][0]                \n",
      "                                                                 lstm_2[0][1]                     \n",
      "                                                                 lstm_2[0][2]                     \n",
      "__________________________________________________________________________________________________\n",
      "dense (Dense)                   (None, None, 2000)   514000      lstm_3[0][0]                     \n",
      "==================================================================================================\n",
      "Total params: 3,505,104\n",
      "Trainable params: 3,505,104\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.layers import Input, LSTM, Embedding, Dense, Concatenate\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "\n",
    "embedding_dim = 128\n",
    "hidden_size = 256\n",
    "\n",
    "# 인코더\n",
    "encoder_inputs = Input(shape=(textMaxLen,))\n",
    "\n",
    "# 인코더의 임베딩 층\n",
    "enc_emb = Embedding(srcVocab, embedding_dim)(encoder_inputs)\n",
    "\n",
    "# 인코더의 LSTM 1\n",
    "encoder_lstm1 = LSTM(hidden_size, return_sequences=True, return_state=True ,dropout = 0.4, recurrent_dropout = 0.4)\n",
    "encoder_output1, state_h1, state_c1 = encoder_lstm1(enc_emb)\n",
    "\n",
    "# 인코더의 LSTM 2\n",
    "encoder_lstm2 = LSTM(hidden_size, return_sequences=True, return_state=True, dropout=0.4, recurrent_dropout=0.4)\n",
    "encoder_output2, state_h2, state_c2 = encoder_lstm2(encoder_output1)\n",
    "\n",
    "# 인코더의 LSTM 3\n",
    "encoder_lstm3 = LSTM(hidden_size, return_state=True, return_sequences=True, dropout=0.4, recurrent_dropout=0.4)\n",
    "encoder_outputs, state_h, state_c= encoder_lstm3(encoder_output2)\n",
    "\n",
    "# 디코더\n",
    "decoder_inputs = Input(shape=(None,))\n",
    "\n",
    "# 디코더의 임베딩 층\n",
    "dec_emb = Embedding(tarVoc, embedding_dim)(decoder_inputs)\n",
    "\n",
    "# 디코더의 LSTM\n",
    "decoder_lstm = LSTM(hidden_size, return_sequences = True, return_state = True, dropout = 0.4, recurrent_dropout=0.2)\n",
    "decoder_outputs, _, _ = decoder_lstm(dec_emb, initial_state = [state_h, state_c])\n",
    "\n",
    "# 디코더의 출력층\n",
    "decoder_softmax_layer = Dense(tarVoc, activation = 'softmax')\n",
    "decoder_softmax_outputs = decoder_softmax_layer(decoder_outputs) \n",
    "\n",
    "# 모델 정의\n",
    "model = Model([encoder_inputs, decoder_inputs], decoder_softmax_outputs)\n",
    "model.summary()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_1\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_1 (InputLayer)            [(None, 50)]         0                                            \n",
      "__________________________________________________________________________________________________\n",
      "embedding (Embedding)           (None, 50, 128)      896000      input_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "lstm (LSTM)                     [(None, 50, 256), (N 394240      embedding[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "input_2 (InputLayer)            [(None, None)]       0                                            \n",
      "__________________________________________________________________________________________________\n",
      "lstm_1 (LSTM)                   [(None, 50, 256), (N 525312      lstm[0][0]                       \n",
      "__________________________________________________________________________________________________\n",
      "embedding_1 (Embedding)         (None, None, 128)    256000      input_2[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "lstm_2 (LSTM)                   [(None, 50, 256), (N 525312      lstm_1[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "lstm_3 (LSTM)                   [(None, None, 256),  394240      embedding_1[0][0]                \n",
      "                                                                 lstm_2[0][1]                     \n",
      "                                                                 lstm_2[0][2]                     \n",
      "__________________________________________________________________________________________________\n",
      "attention_layer (AttentionLayer ((None, None, 256),  131328      lstm_2[0][0]                     \n",
      "                                                                 lstm_3[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "concat_layer (Concatenate)      (None, None, 512)    0           lstm_3[0][0]                     \n",
      "                                                                 attention_layer[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "dense_1 (Dense)                 (None, None, 2000)   1026000     concat_layer[0][0]               \n",
      "==================================================================================================\n",
      "Total params: 4,148,432\n",
      "Trainable params: 4,148,432\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n",
      "WARNING:tensorflow:From C:\\Users\\student\\Anaconda3\\lib\\site-packages\\tensorflow_core\\python\\ops\\math_grad.py:1424: where (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.where in 2.0, which has the same broadcast rule as np.where\n",
      "Train on 52722 samples, validate on 13181 samples\n",
      "52722/52722 [==============================] - 1730s 33ms/sample - loss: 2.4906 - val_loss: 2.2321\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "ename": "NameError",
     "evalue": "name 'dec_emb_layer' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-49-28d3870c4e03>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     43\u001b[0m \u001b[0mdecoder_state_input_c\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mInput\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mhidden_size\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     44\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 45\u001b[1;33m \u001b[0mdec_emb2\u001b[0m\u001b[1;33m=\u001b[0m \u001b[0mdec_emb_layer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdecoder_inputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     46\u001b[0m \u001b[1;31m# 문장의 다음 단어를 예측하기 위해서 초기 상태(initial_state)를 이전 시점의 상태로 사용. 이는 뒤의 함수 decode_sequence()에 구현\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     47\u001b[0m \u001b[1;31m# 훈련 과정에서와 달리 LSTM의 리턴하는 은닉 상태와 셀 상태인 state_h와 state_c를 버리지 않음.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'dec_emb_layer' is not defined"
     ]
    }
   ],
   "source": [
    "import urllib.request\n",
    "urllib.request.urlretrieve(\"https://raw.githubusercontent.com/thushv89/attention_keras/master/layers/attention.py\", filename=\"attention.py\")\n",
    "from attention import AttentionLayer\n",
    "\n",
    "# 어텐션 층(어텐션 함수)\n",
    "attn_layer = AttentionLayer(name='attention_layer')\n",
    "attn_out, attn_states = attn_layer([encoder_outputs, decoder_outputs])\n",
    "\n",
    "# 어텐션의 결과와 디코더의 hidden state들을 연결\n",
    "decoder_concat_input = Concatenate(axis = -1, name='concat_layer')([decoder_outputs, attn_out])\n",
    "\n",
    "# 디코더의 출력층\n",
    "decoder_softmax_layer = Dense(tarVoc, activation='softmax')\n",
    "decoder_softmax_outputs = decoder_softmax_layer(decoder_concat_input)\n",
    "\n",
    "# 모델 정의\n",
    "model = Model([encoder_inputs, decoder_inputs], decoder_softmax_outputs)\n",
    "model.summary()\n",
    "\n",
    "model.compile(optimizer='rmsprop', loss='sparse_categorical_crossentropy')\n",
    "\n",
    "es = EarlyStopping(monitor='val_loss', mode='min', verbose=1, patience = 2)\n",
    "history = model.fit([xTrain, yTrain[:,:-1]], yTrain.reshape(yTrain.shape[0], yTrain.shape[1], 1)[:,1:] \\\n",
    "                  ,epochs=1, callbacks=[es], batch_size = 256, validation_data=([xTest, yTest[:,:-1]], \\\n",
    "                  yTest.reshape(yTest.shape[0], yTest.shape[1], 1)[:,1:]))\n",
    "#epochs = 50으로 교체\n",
    "plt.plot(history.history['loss'], label='train')\n",
    "plt.plot(history.history['val_loss'], label='test')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "\n",
    "\n",
    "src_index_to_word = srcToken.index_word # 원문 단어 집합에서 정수 -> 단어를 얻음\n",
    "tar_word_to_index = tarToken.word_index # 요약 단어 집합에서 단어 -> 정수를 얻음\n",
    "tar_index_to_word = tarToken.index_word # 요약 단어 집합에서 정수 -> 단어를 얻음\n",
    "\n",
    "# 인코더 설계\n",
    "encoder_model = Model(inputs=encoder_inputs, outputs=[encoder_outputs, state_h, state_c])\n",
    "\n",
    "# 이전 시점의 상태들을 저장하는 텐서\n",
    "decoder_state_input_h = Input(shape=(hidden_size,))\n",
    "decoder_state_input_c = Input(shape=(hidden_size,))\n",
    "\n",
    "dec_emb2= dec_emb_layer(decoder_inputs) \n",
    "# 문장의 다음 단어를 예측하기 위해서 초기 상태(initial_state)를 이전 시점의 상태로 사용. 이는 뒤의 함수 decode_sequence()에 구현\n",
    "# 훈련 과정에서와 달리 LSTM의 리턴하는 은닉 상태와 셀 상태인 state_h와 state_c를 버리지 않음.\n",
    "decoder_outputs2, state_h2, state_c2 = decoder_lstm(dec_emb2, initial_state=[decoder_state_input_h, decoder_state_input_c])\n",
    "\n",
    "# 어텐션 함수\n",
    "decoder_hidden_state_input = Input(shape=(textMaxLen, hidden_size))\n",
    "attn_out_inf, attn_states_inf = attn_layer([decoder_hidden_state_input, decoder_outputs2])\n",
    "decoder_inf_concat = Concatenate(axis=-1, name='concat')([decoder_outputs2, attn_out_inf])\n",
    "\n",
    "# 디코더의 출력층\n",
    "decoder_outputs2 = decoder_softmax_layer(decoder_inf_concat) \n",
    "\n",
    "# 최종 디코더 모델\n",
    "decoder_model = Model(\n",
    "    [decoder_inputs] + [decoder_hidden_state_input,decoder_state_input_h, decoder_state_input_c],\n",
    "    [decoder_outputs2] + [state_h2, state_c2])\n",
    "\n",
    "def decode_sequence(input_seq):#(1,50)\n",
    "    # 입력으로부터 인코더의 상태를 얻음\n",
    "    states_value = encoder_model.predict(input_seq)\n",
    "    # <SOS>에 해당하는 원-핫 벡터 생성\n",
    "    target_seq = np.zeros((1, 1, tarVoce))\n",
    "    target_seq[0, 0, tar_to_index['\\t']] = 1.\n",
    "\n",
    "    stop_condition = False\n",
    "    decoded_sentence = \"\"\n",
    "    while not stop_condition: #stop_condition이 True가 될 때까지 루프 반복\n",
    "        # 이점 시점의 상태 states_value를 현 시점의 초기 상태로 사용\n",
    "        output_tokens, h, c = decoder_model.predict([target_seq] + states_value)\n",
    "        sampled_token_index = np.argmax(output_tokens[0, -1, :])\n",
    "        sampled_char = index_to_tar[sampled_token_index]\n",
    "        decoded_sentence += sampled_char\n",
    "\n",
    "        # <eos>에 도달하거나 최대 길이를 넘으면 중단.\n",
    "        if (sampled_char == '\\n' or\n",
    "           len(decoded_sentence) > maxTarLen):\n",
    "            stop_condition = True\n",
    "\n",
    "        # 길이가 1인 타겟 시퀀스를 업데이트 합니다.\n",
    "        target_seq = np.zeros((1, 1, tar_vocab_size))\n",
    "        target_seq[0, 0, sampled_token_index] = 1.\n",
    "\n",
    "        # 상태를 업데이트 합니다.\n",
    "        states_value = [h, c]\n",
    "\n",
    "    return decoded_sentence\n",
    "\n",
    "# 원문의 정수 시퀀스를 텍스트 시퀀스로 변환\n",
    "def seq2text(input_seq):\n",
    "    temp=''\n",
    "    for i in input_seq:\n",
    "        if(i!=0):\n",
    "            temp = temp + src_index_to_word[i]+' '\n",
    "    return temp\n",
    "\n",
    "# 요약문의 정수 시퀀스를 텍스트 시퀀스로 변환\n",
    "def seq2summary(input_seq):\n",
    "    temp=''\n",
    "    for i in input_seq:\n",
    "        if((i!=0 and i!=target_word_index['sostoken']) and i!=target_word_index['eostoken']):\n",
    "            temp = temp + tar_index_to_word[i] + ' '\n",
    "    return temp\n",
    "\n",
    "for i in range(500, 1000):\n",
    "    print(\"원문 : \",seq2text(xTest[i]))\n",
    "    print(\"실제 요약문 :\",seq2summary(yTest[i]))\n",
    "    print(\"예측 요약문 :\",decode_sequence(xTest[i].reshape(1, textMaxLen)))\n",
    "    print(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import load_model\n",
    "model2 = load_model(\"C:/Users/student/Downloads/Python_JP/review_seq2seq_Multi_LSTM\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
